### 策略梯度
强化学习中的方法通常以学习处于特定状态或在状态中采取特定行动的价值(状态值或动作值)为中心，策略梯度方法提供另一种视角。我们不学习价值函数，而是直接学习策略本身。我们将策略定义为一个参数化的函数，我们将其记作$\pi_{\theta}(a|s)$，它输出在状态$s$中采取行动$a$的概率。此函数的参数由$\theta$表示。

主要思想是直接调整参数$\theta$，以提升策略的质量。在强化学习中，我们的目标通常是最大化期望的总折现回报。我们可以定义一个性能指标，通常表示为$J(\theta)$，它代表通过遵循策略$\pi_{\theta}$所获得的期望回报。对于回合制任务，这通常是起始状态的价值：
$$J(\theta) = V^{\pi_{\theta}}(s_{0}) = E_{\pi_{\theta}}[\sum_{t=0}^{T}{\gamma^{t} * R_{t+1}| S_{0} = s_{0}}]$$

我们的目标是找到使这个性能指标$J(\theta)$最大化的参数$\theta$。

#### 为何直接学习策略
直接对策略进行参数化带来几个好处：

1. 连续动作空间：像Q学习这样的基于价值的方法难以处理连续动作。如果动作空间是连续的，则在每一步都需要一个$Q(s,a)$最大化的动作$a$。策略梯度方法可以更自然地处理这个问题。参数化的策略$\pi_{\theta}(a|s)$可以设计为直接输出动作的概率分布的参数或连续动作本身。

2. 随机策略：基于价值的方法通常隐含地学习确定性策略（例如，通过总是选择具有最大Q值的动作）。策略梯度方法可以通过输出概率$\pi_{\theta}(a|s)$明确地学习随机策略。

3. 更简单的表示：策略函数比价值函数更容易近似。


为了最大化性能，我们使用梯度上升，策略参数$\theta$的基本更新规则是：
$$\theta_{k+1} = \theta_{k} + a*\nabla_{\theta}(J_{\theta})$$
其中，$\theta_{t}$是在第k次迭代时的策略参数，$\nabla_{\theta}(J_{\theta})$是性能指标关于策略参数$\theta$的梯度。

#### 策略梯度目标推导
![](/assets/img/pg_object.png "RL object")
$$p_{\theta}(s_{1}, a_{1}, s_{2}, a_{2}, ......, s_{T}, a_{T}) = p(s_{1})\prod_{t=1}^{T}{\pi_{\theta}(a_{t}|s_{t})p(s_{t+1}|s_{t}, a_{t})}$$

$$\theta^{*} = \arg \max_{\theta}E_{\tau \in p_{\theta}(\tau)}\Bigg[\sum_{t}{r(s,a)}\Bigg]$$

$$p_{\theta}(\tau)\nabla_{\theta}\log p_{\theta}(\tau) = p_{\theta}(\tau)\frac{\nabla_{\theta}p_{\theta}(\tau)}{p_{\theta}(\tau)} = \nabla_{\theta}p_{\theta}(\tau)$$

$$r(\tau) = \sum_{t=1}^{T}{r(s_{t}, a_{t})}$$

$$J(\theta) = E_{\tau \in p_{\theta}(\tau)}\bigg[r(\tau)\bigg] = \int p_{\theta}(\tau)r(\tau) d_{\tau}$$

$$\nabla_{\theta}J(\theta) = \int \nabla_{\theta}p_{\theta}(\tau)r(\tau)d_{\tau} = \int p_{\theta}(\tau)\nabla_{\theta}\log p_{\theta}(\tau)r(\tau)d_{\tau} = E_{\tau \in p_{\theta}(\tau)}\Bigg[\nabla_{\theta}\log p_{\theta}(\tau)r(\tau)\Bigg]$$

$$\log p_{\theta}(s_{1}, a_{1}, s_{2}, a_{2}, ......, s_{T}, a_{T}) = \log p(s_{1}) + \sum_{t=1}^{T}{\log \pi_{\theta}(a_{t}|s_{t})} + \log p(s_{t+1}|s_{t}, a_{t})$$

$$\nabla_{\theta}J(\theta)  =  E_{\tau \in p_{\theta}(\tau)}\Bigg[\Bigg(\sum_{t=1}^{T}{\nabla_{\theta}\log \pi_{\theta}(a_{t}|s_{t})}\Bigg)\Bigg(\sum_{t=1}^{T}{r(s_{t}, a_{t})}\Bigg)\Bigg]$$

#### REINFORCE
REINFORCE 也称为蒙特卡洛策略梯度，它提供了一种直接的方式，根据与环境交互的结果来调整策略参数$\theta$，REINFORCE的核心理念很直观：增加导致良好结果的动作的概率，并减少导致不良结果的动作的概率。

REINFORCE 是一种蒙特卡洛方法，因为它只在观察到整个回合的完整回报后才学习。它会等到回合结束，计算从每个状态$s_{t}$开始获得的回报$G_{t}$, 通常是初始状态的期望回报，然后根据这些回报更新策略参数。
$$G_{t} = r_{t+1} + \gamma*r_{t+2} + .... + \gamma^{T-t-1}*r_{T}$$
对于单个回合，在回合结束后应用于参数$\theta$的更新，考虑每个时间步$t$，为：
$$\theta \leftarrow \theta + a * \sum_{t=1}^{T}{\nabla_{\theta}\log \pi_{\theta}(a_{t}|s_{t})} * G_{t}$$
REINFORCE 是一种蒙特卡洛方法，实际上会对多个trajectory做梯度的平均。
$$\nabla_{\theta}J(\theta) \approx \frac{1}{N}\sum_{i=1}^{N}\Bigg(\sum_{t=1}^{T}{\nabla_{\theta}\log \pi_{\theta}(a_{i,t}|s_{i,t})}\Bigg)\Bigg(G_{(i,t)}\Bigg)$$

策略$\pi_{\theta}(a|s)$需要是一个函数，它以状态$s$和参数$\theta$作为输入，并输出每个可能动作$a$的概率。对于离散动作空间，一种常见的方法是使用神经网络输出每个动作的分数（logits），然后通过$softmax$函数将这些分数转换为概率。


优点：
1. 能够自然地学习随机策略。
2. 通过适当参数化策略来处理连续动作空间

缺点：
1. 高方差： 最大的缺点。更新取决于蒙特卡洛回报$G_{t}$，即使使用相同的策略，仅仅由于环境或策略本身的随机性，它在不同回合之间也可能显著变化。这种高方差使得学习缓慢且不稳定。
2. 样本效率低下： 学习只在回合结束时发生，可能需要许多回合才能取得显著进展，特别是对于长回合。


##### 基线
为了缓解高方差问题，我们可以通过从回报$G_{t}$中减去一个基线$b(s_{t})$来修改策略梯度更新。

修改后的更新规则变为：
$$\theta \leftarrow \theta + a * \sum_{t=1}^{T}{\nabla_{\theta}\log \pi_{\theta}(a_{t}|s_{t})} * \bigg(G_{t} - b\Bigg)$$
$$b = \frac{1}{N}{\sum_{i=1}^{N}{r(\tau)}}$$

基线的一个重要属性是减去它不会改变梯度更新的期望值，它在降低方差的同时不引入偏差。平均奖励虽然不是最好的基线，但其足够简单，也能取得胜利很不错的结果。


### 策略梯度面临的挑战
策略梯度方法，例如 REINFORCE，提供了一种直接优化参数化策略的方式。然而，这些方法在实践中，尤其是在复杂环境中，常常面临显著的实际困难。回顾源自策略梯度定理的基本策略梯度更新。对于表示预期总回报的目标函数$J(\theta)$，梯度估计如下:
$$\nabla_{\theta}J(\theta)  =  E_{\tau \in p_{\theta}(\tau)}\Bigg[\Bigg(\sum_{t=1}^{T}{\nabla_{\theta}\log \pi_{\theta}(a_{t}|s_{t})}\Bigg)G_{t}\Bigg]$$
其中$\tau$是遵循策略$\pi_{\theta}$生成的轨迹$(s_{1}, a_{1}, ..., s_{T}, a_{T})$，而$G_{t}$是从时间步$t$开始的折扣回报。实际操作中，这个期望值通过蒙特卡洛采样来近似，即对使用当前策略$\pi_{\theta}$收集的多个轨迹的梯度分量进行平均。

##### 高方差
REINFORCE 这样的基本策略梯度方法最主要的问题是梯度估计的高方差，这种方差直接源于使用蒙特卡洛回报$G_{t}$作为策略梯度项$\nabla_{\theta}\log \pi_{\theta}(a_{t}|s_{t})$的缩放因子。

高方差可能回导致：
1. 收敛缓慢：学习需要对许多轨迹进行平均以获得可靠的信号，这大大减缓了学习过程。；
2. 不稳定：有噪声的更新可能导致策略性能剧烈波动甚至发散。；
3. 敏感性：学习过程对学习率和初始化参数的选择变得高度敏感；

##### 样本效率低下
高方差直接导致了样本效率低下。因为每个采样轨迹都提供了如此嘈杂的梯度估计，必须在当前策略下收集大量轨迹才能获得一个合理准确的更新方向，这使得学习在交互时间和数据需求方面变得昂贵。

此外，标准 REINFORCE 通常会等到回合结束才计算回报$G_{t}$并执行更新。这意味着学习信号被延迟，并且中间奖励的信息未能像时序差分 (TD) 方法那样及时使用。

##### 信用分配问题
 REINFORCE 算法根据轨迹的总回报$G_{t}$（通常只是$G_{0}$）来更新轨迹中所有已采取动作的概率。如果一条轨迹产生了很高的总回报，那么该轨迹中的所有动作都会得到强化，即使其中一些特定动作实际上是有害的，但被后来的幸运情况或良好动作所抵消。相反，单个导致整体回报不佳的坏动作可能会不公平地惩罚之前的良好动作。

使用从当前时间步开始的回报$G_{t}$，而不是总回报$G_{0}$，通过仅根据后续奖励强化动作来帮助缓解此问题。

这些挑战，即高方差、样本效率低下和困难的信用分配，使得基本策略梯度形式需要改进。