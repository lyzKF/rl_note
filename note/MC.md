### 蒙特卡罗
蒙特卡洛（MC）方法，这是一类免模型的强化学习算法。蒙特卡洛方法直接从经验片段中学习，无需预先知道环境的运行方式。

动态规划方法非常依赖于拥有一个完整的环境模型。这些方法要求了解状态转移概率$p(s_{t+1}|s_{t},a_{t})$来计算期望值并找到最优策略。然而，在许多实际问题中，这样的模型不可得，或者过于复杂而难以精确描述。代理如何在不知道游戏规则的前提下，学会做出好的决策呢？

蒙特卡洛方法的核心思想很简单：它们基于在许多回合中访问某个状态（或状态-动作对）后观察到的平均回报来估计价值函数。它们的工作方式是等待整个回合完成。只有这样，才能计算出该回合中访问的每个状态之后的实际回报。
回顾回报$G_{t}$的定义，它是从时间步$t$开始的总折扣奖励：$$G_{t} = \sum_{t=0}^{T}{\gamma^{t}*r_{t+1}}$$，其中$T$是回合的终止时间。因为 MC 方法需要直到回合终止的完整奖励序列才能计算$G_{t}$，它们只能直接应用于回合型任务，即那些保证最终会终止的任务。

一个回合结束，我们就会得到一个$(s_{t}, a_{t}, r_{t+1})$元组的样本序列，我们可以回溯计算该回合中每个时间步$t$的观测回报$G_{t}$。


#### MC估计状态值
使用MC方法的第一步通常是预测，这意味着估计给定策略$\pi$的状态值函数$V^{\pi}(s_{t})$。状态值函数$V^{\pi}(s_{t})$表示从状态$s$开始并随后遵循策略$\pi$所获得累积折扣回报的预期。

为了获得这些样本，我们需要按照策略$\pi$运行完整的episode，episode是状态、动作和回报的序列，从初始状态开始并终止于一个结束状态。MC通过平均在多个episode中访问状态$s$后观察到的回报$G_{t}$来估计$V^{\pi}(s_{t})$。

实现此方法的一个实用方式是使用增量更新规则，当状态$s$观察到新的回报$G$时：
1. 增加计数：$N(s) \leftarrow N(s) + 1$
2. 更新值估计：$V(s) \leftarrow V(s) + \frac{1}{N)(s)} * (G - V(s))$



#### MC估计动作值
蒙特卡洛（MC）方法通过平均访问状态$s$后观察到的回报，可以估算给定策略$\pi$的状态价值函数$V^{\pi}(s_{t})$。这个过程被称为MC预测或评估，它告诉我们在当前策略下处于特定状态有多好。然而，强化学习的最终目的通常是找到使所有状态的期望回报最大化的最优策略$\pi^{*}$。
为了使用MC方法实现控制，估算状态价值函数$V^{\pi}(s_{t})$通常是不够的。因为改进策略需要了解在给定状态下选择不同动作的价值。如果我们只知道$V^{\pi}(s_{t})$，在不知道环境动态的情况下，我们无法轻易判断哪个动作$a$能带来最好的结果。如果我们对状态$s$中所有可用动作$a$都有准确的$Q^{\pi}(s,a)$估算，那么选择最佳动作就简单直接了：我们只需选择具有最高估算$Q$值的动作。$$\pi^{*}(s) = argmax_{a} Q(s,a)$$
因此，对于使用MC方法的免模型控制，我们将重心从估算$V^{\pi}(s)$转到$Q^{\pi}(s,a)$。

核心思想与MC预测保持一致：我们从完整的经验片段中学习，平均每个访问的状态-动作对$(s,a)$的回报。实现此方法的一个实用方式是使用增量更新规则，当状态$s$观察到新的回报$G$时：
1. 增加计数：$N(s,a) \leftarrow N(s,a) + 1$
2. 更新值估计：$Q(s,a) \leftarrow Q(s,a) + \frac{1}{N(s,a)} * (G - Q(s,a))$

通常，对于非平稳问题（环境或策略可能发生变化），会使用恒定步长$a$代替$\frac{1}{N(s,a)}$。

#### 试探的挑战
我们如何使用它来找到最优策略$V^{*}$呢？自然的想法是遵循广义策略迭代（GPI）的模式，它涉及在策略评估（估算当前策略的价值函数）和策略改进（使策略相对于当前价值函数估算变为贪婪策略）之间交替进行。

使用MC方法，这看起来会像：
1. 策略评估：在策略$\pi$下运行多个片段，并通过平均状态-动作对的回报来使用MC估算$Q^{\pi}(s, a)$
2. 策略改进：更新策略$\pi$，使其相对于估算的$Q$值变为贪婪策略：对所有$s$，$\pi(s) \leftarrow argmax_{a}{Q(s, a)}$
3. 重复此过程，直到策略和价值函数稳定下来。

如果策略$\pi$在早期就变得确定且贪婪（例如，它总是从状态$s$选择动作$a_{1}$，那么我们将只观察到对$(s_{1}, a_{1})$对的回报。我们将永远无法获取对状态$s$中其他可能更好的动作（如$a_{2}, a_{3}$的经验（因此也无法更新$Q$值估算）。智能体可能会陷入次优循环，因为它从未尝试其他选项。为保证MC方法收敛到最优策略，我们需要保证所有状态-动作对都能持续被访问。

#### 柔性策略
核心思想很简单：智能体不应总是选择被认为最好的动作，而应该偶尔随机选择一个动作。这确保了，随着时间的推移，所有动作都能在所有被访问过的状态中被采样到。

$\epsilon$-柔性策略指的是这样一种策略：它在给定状态$s$下选择任意动作$a$时，分配一个最小概率$\frac{\epsilon}{A(s)}$，$A(s)$是该状态下可用动作的数量，$\epsilon$是一个很小的正数。具体来说，对于一个$\epsilon$-柔性策略$\pi$：
1. 当前根据动作值函数$Q(s, a)$，看起来最好的动作$a^{*}$，$a^{*} = argmax_aQ(s,a)$，以$1-\epsilon +\frac{\epsilon}{A(s)}$的概率选中。
2. 所有其他非贪婪动作以$\frac{\epsilon}{A(s)}$的概率被选择。
