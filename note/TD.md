### 时序差分
蒙特卡罗的一个主要限制：学习只能在回合结束后才进行。对于非常长甚至连续的任务（永不结束的任务），等待“最终结果”是不实际或不可能的。时序差分(TD)不需要等到回合结束才能计算最终的回报$G_{t}$，而是根据获得的即时奖励$r_{t+1}$以及下一个状态$s_{t+1}$的当前估计价值来更新状态$s_{t}$的估计值。

设想当前状态$s_{t}$，采取动作$a_{t}$，获得奖励$r_{t+1}$，并达到下一个状态$s_{t+1}$：最简单形式TD(0)，仅使用下一步的信息形成一个目标。即时奖励$r_{t+1}$加上下一状态价值的折扣当前估计，即$\gamma * V(s_{t+1})$，在时间$t$进行更新的目标是：$$r_{t+1} + \gamma * V(s_{t+1})$$

这种部分基于另一个已学习估计来更新一个估计的过程称为自举，TD 学习使用自举，因为对$V(s_{t})$的更新依赖于现有的$V(s_{t+1})$估计。

#### TD(0)估计状态值
回想一下，蒙特卡罗方法是根据从状态$s_{t}$开始的完整观测回报$G_{t}$，来更新状态$s_{t}$的估计值$V_{s_{t}}$。$$V(s_{t}) \leftarrow V(s_{t}) + \alpha * (G_{t} - V(s_{t}))$$
TD(0)在执行动作$a_{t}$并获得奖励$r_{t+1}$后，观察到状态从$s_{t}$转移到状态$s_{t+1}$时，会立即进行更新。TD(0)不使用完整的返回值$G_{t}$，而是使用估计的返回值，这个估计是通过即时奖励$r_{t+1}$和下一个状态值$V(s_{t+1})$的当前估计结合起来的。$r_{t+1} + \gamma * V(s_{t+1})$被称为TD目标。

TD(0)对$V(s_{t})$的更新规则是：$$V(s_{t}) \leftarrow V(s_{t}) + \alpha * (r_{t+1} + \gamma * V(s_{t+1}) - V(s_{t}))$$

#### SARSA
如果我们的目标是控制，即找到最优策略，那么仅仅估$V_{\pi}$是不够的。对于控制问题，特别是在没有环境模型的情况下，我们通常需要学习动作价值，$Q_{\pi}(s,a)$。这告诉我们，在状态$s$采取动作$a$之后，并随后遵循策略$\pi$所获得的预期回报。了解$Q$值使智能体能够在每个状态中，通过选择具有最高$Q$值的动作来选取最佳动作。

SARSA这个名称强调了其更新计算中涉及的事件顺序：智能体从一个状态$S_{t}$开始，选择一个动作$a_{t}$，收到一个奖励$r_{t+1}$，并转换到下一个状态$r_{t+1}$，然后根据其当前策略选择下一个动作$a_{t+1}$。这个五元组构成了SARSA更新的根本要素。
SARSA更新规则：与TD(0)一样，SARSA在每个时间步之后执行一次更新。动作价值函数$Q_{\pi}(s,a)$的更新规则如下：
$$Q(s_{t},a_{t}) \leftarrow Q(s_{t},a_{t}) + \alpha * (r_{t+1} + \gamma * Q(s_{t+1},a_{t+1}) - Q(s_{t},a_{t}))$$

SARSA在一个通用的控制循环中使用这个更新规则，旨在随着时间推移改进策略。由于我们需要学习所有动作的价值以找到最佳动作，所遵循的策略必须能够进行新尝试。一种常见的方法是使用基于当前$Q$值估计的ϵ-贪婪策略。这意味着智能体通常选择具有最高Q值的动作（利用），但偶尔也会以ϵ的概率选择一个随机动作（进行尝试）。

#### Q-learning
Q-learning直接针对最优动作-价值函数，$Q^{*}(s,a)$，无论智能体实际使用何种策略来对环境进行尝试。Q学习中的目标策略是相对于当前动作-价值估计的贪婪策略，而行为策略可以是更具试探性的，例如epsilon-贪婪策略。

Q-learning更新规则：Q-learning的核心在于其更新规则。在状态$s_{t}$中采取动作$a_{t}$并观察到奖励$r_{t+1}$和下一状态$s_{t+1}$之后，Q学习使用以下规则更新其估计值$Q(s_{t},a_{t})$：
$$Q(s_{t},a_{t}) \leftarrow Q(s_{t},a_{t}) + \alpha * (r_{t+1} + \gamma * max_{a}Q(s_{t+1},a) - Q(s_{t},a_{t}))$$