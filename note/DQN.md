### deep Q-Network
Q-learning与deep-learning的结合。设想这样的任务，例如直接从屏幕像素玩视频游戏，或者根据高维传感器输入控制机器人。这里的状态空间极其庞大，最优策略可能取决于输入中复杂的非线性模式。为了处理大规模且包含复杂非线性模式的问题，深度神经网络展现出其作用。神经网络擅长从高维输入中学习复杂的层次化特征和非线性映射。通过使用神经网络来逼近动作价值函数$Q(s, a)$，我们能够在视觉信息丰富或状态表示高度复杂的环境中，学习到有效的策略。

这种方法为深度Q学习（Deep Q-Learning），所用的神经网络通常被称为深度Q网络（DQN）。我们不再将Q值存储在表格中或使用简单的线性函数，而是使用一个神经网络，其参数（权重和偏置）统称为$\theta$。这个网络将状态表示$s$作为输入，并输出该状态下每个可能动作$a$的$Q$值估计。目标是训练网络，这意味着我们需要找到参数$\theta$，使网络的输出$Q(s,a, \theta)$能够良好逼近最优动作价值函数$Q^{*}(s,a,\theta)$。

标准的Q学习更新依赖于TD目标：$y = r + \gamma * max_{a^{'}}Q(s^{'},a^{'})$，在深度Q学习中，我们将网络的训练视为一个监督学习问题。对于给定的输入$(S,A,R,S^{'})$，网络预测当前的Q值$Q(S,A,\theta)$。
理想情况下，我们希望网络的预测$Q(S,A,\theta)$与这个目标$y$匹配。我们可以定义一个损失函数，用于衡量预测与目标之间的差异。一个常见的选择是均方误差（MSE）：
$$
\begin{align}
L(\theta) = & (y - Q(S,A,\theta))^{2} \\
L(\theta) = & (r + \gamma * max_{a^{'}}Q(S^{'},A^{'},\theta) - Q(S,A,\theta))^{2}
\end{align}
$$

在标准RL循环中直接应用神经网络带来了一系列特有的挑战。代理与环境交互生成的数据具有序列化、相关联的特性，这违反了监督学习中常做的独立性假设。此外，目标值$y$本身依赖于正在更新的网络参数$\theta$，这可能导致训练期间的不稳定性。

#### 经验回放
一个回合中连续的经验高度相关，状态$s_{t+1}$依赖于状态$s_{t}$和动作$a_{t}$，在相关样本序列上进行训练可能导致网络收敛到不佳的局部最优值，或导致学习到的参数$\theta$出现振荡。

经验回放的核心思想简单而有效：agent不会立即使用最新的经验进行训练，而是将其经验存储在一个大型内存缓冲区中，通常称为回放缓冲区或回放内存。通常以元组形式存储：$(s_{t}, a_{t}, s_{t+1}, r_{t+1})$。
当新的经验到来时，它们会被添加到缓冲区中，如果缓冲区已满，则可能覆盖最旧的经验。

在学习阶段，不是使用最新的转换，而是从回放缓冲区中随机抽取一个小批量的数据。然后使用这些随机抽取的数据对Q网络的参数$\theta$进行梯度下降更新。

#### 固定Q网络
$$
\begin{align}
L(\theta) = & (r + \gamma * max_{a^{'}}Q(S^{'},A^{'},\theta) - Q(S,A,\theta))^{2}
\end{align}
$$
通过Deep Q-learning的损失函数定义可知，另一个主要的不稳定性来源在于时序差分 (TD) 更新中使用的目标值不断变化。当我们执行梯度下降来更新$\theta$时，我们本质上是在追逐一个移动的目标。随着网络权重$\theta$在每一步中变化，目标值本身也会移动。

目标网络$\theta^{-}$本质上是在线Q网络的克隆，损失函数计算如下：
$$
\begin{align}
L(\theta) = & (r + \gamma * max_{a^{'}}Q(S^{'},A^{'},\theta^{-}) - Q(S,A,\theta))^{2}
\end{align}
$$

参数更新： 只有在线网络参数$\theta$会通过使用这个损失进行梯度下降来更新。目标网络参数$\theta^{-}$在这些更新期间保持不变。
在固定数量的训练步骤后，在线网络的权重会被复制到目标网络：$\theta^{-} \leftarrow \theta$。
更新目标网络的频率是一个需要选择的超参数；典型值可能从数百到数千步不等，具体取决于特定问题。

##### 软更新
除了周期性的“硬”复制之外，另一种方法是“软”更新，也称为Polyak平均。此方法在每个训练步骤中，将目标网络参数缓慢地更新到在线网络参数方向：
$$\theta^{-} \leftarrow \tau*\theta  + (1 - \tau) * \theta^{-}$$
这里$\tau$是一个小的常数($\tau = 0.001$)。与周期性的硬更新相比，这种方法使得目标网络随时间变化更加平滑。

#### 双重深度Q网络(DDQN)
DQN中使用的标准Q学习更新可能会遇到一个主要问题：Q值过高估计。双重深度Q网络，通过在计算目标时，将最优动作的选择与该动作值的评估分离开来，直接解决了这个过高估计问题。DDQN使用在线网络$\theta_{t}$来为下一个状态$s_{t+1}$选择最优动作，然后使用目标网络$\theta^{-}_{t}$来评估该特定所选动作的Q值。DDQN中的目标值计算变为：$$y_{t}^{DDQN} = r_{t+1} + \gamma*Q(s_{t+1}, \arg \max_{a^{'}}Q(s_{t+1}, a^{'}, \theta_{t}); \theta^{-}_{t})$$

动作选择：我们使用当前在线网络$Q(s_{t+1}, a^{'}, \theta_{t})$来找到它认为在下一个状态$s_{t+1}$中能使Q值最大化的动作$a^{*}$。
动作评估：使用目标网络$Q(;\theta^{-}_{t})$来获取在线网络选择的那个特定动作$a^{*}$的Q值估计。

在线网络$\theta_{t}$和目标网络$\theta^{-}_{t}$是不同的参数集，虽然两个网络都可能对某些动作存在噪声和潜在的过高估计，但两个网络同时高估同一次优动作值的可能性较小。

#### 优先经验回放(PER)
标准经验回放将回放缓冲区中存储的所有转移视为同等重要。在抽取用于训练的小批量时，每个样本$(S,A,R,S^{'})$被选中的概率是均等的。然而直观上看，并非所有经验都提供相同的学习潜力。优先经验回放建立在这一想法之上，通过根据转移的重要性进行采样，优先选择那些智能体能学到最多的转移。其主要思想是使用时序差分（TD）误差的大小作为衡量样本提供多少有益信息的替代指标。
使用目标网络参数$\theta^{-}$和当前网络参数$\theta$，计算样本$(S,A,R,S^{'})$的TD误差：
$$\delta = r + \gamma * \max_{a^{'}}Q(S^{'},A^{'},\theta^{-}) - Q(S,A,\theta)$$
较大的绝对TD误差$|\delta|$表明当前Q函数可能不准确的情况，因此提供了有力的学习信号。

在PER中，我们不采用均匀采样，而是为回放缓冲区中的每个样本分配一个优先级$p_{i}$，这种优先级的一个常见选择与绝对TD误差直接相关：$$p_{i} = |\delta_{i}| + \epsilon$$
其中，$\epsilon$是一个小的正常量，用于以保证TD误差为零的转移仍然有非零的采样概率。一旦分配了优先级，我们就将其转换为每个样本的采样概率：$$p_{i} = \frac{p_{i}^{a}}{\sum_{k}{p_{k}^{a}}}$$
指数$a \geq 0$控制优先级的程度。如果$a=0$，那么$p_{i} = \frac{1}{N}$，即均匀采样。如果$a=1$，我们得到基于$p_{i}$的直接比例优先级。如果$a$在0到1之间的值，允许在均匀采样和完全优先级之间进行插值，提供了一种调节对高优先级样本关注度的方式。


### 分布强化学习
标准的深度Q网络（DQN）及其变体主要侧重于估计预期的未来折扣回报，$
Q^{\pi}(s_{t},a_{t}) = E_{\pi}{[r_{t+1} + \gamma * G_{t+1}|S_{t}=s_{t}, A_{t}=a_{t}]}$。这种期望将状态$s$中采取动作$a$的潜在结果压缩为一个单一的标量值。然而，这种压缩可能会丢失有关回报分布的变异性和形态的有价值信息。

分布强化学习通过直接建模随机回报$Z(s,a)$，的概率分布来解决此问题，而不仅仅是其期望值$E[Z(s,a)]$。其核心思想是将贝尔曼方程推广到分布，假设$Z(s,a)$是一个随机变量，表示从状态$s$开始，执行动作$a$之后，并按照当前策略持续下去所获得的回报。标准的贝尔曼最优性方程关联的是期望值：$$Q^{*}(s,a) = E\big[r(s,a) + \gamma*\max_{a^{'}}(s^{'}, a^{'})\big]$$
分布式版本关联的是分布本身：$$Z(s,a) =^{D} r(s,a) + \gamma * Z(s^{'}, a^{'})$$
其中，$=^{D}$表示分布上相等，最优的下一动作$a^{'*}$通常是通过最大化下一状态回报分布的期望值来选择的：$a^{'*} = \arg \max_{a^{'}}E\big[Z(s^{'}, a^{'})\big]$

表示和学习潜在的连续概率分布是具有挑战性的。实际算法采用近似方法，例如C51、QR-DQN。学习完整的回报分布提供了多项好处：

- 更丰富学习信号：与单一的期望值相比，分布提供了更详细的信息，可能导致更稳定和有效的学习，尤其是在奖励或状态转移随机的环境中。它有助于区分均值相似但风险特征不同的动作。

- 前沿性能：分布强化学习算法，特别是C51和QR-DQN，在Atari套件等挑战性基准测试中表现出显著的性能提升，是结合了多项DQN改进的Rainbow智能体的重要组成部分。

- 风险感知：拥有回报分布使得明确的风险感知决策成为可能。智能体不再仅仅最大化均值$E\big[Z(s,a)\big]$，还可以优化其他统计量，如特定分位数（例如，最大化10%分位数以实现风险规避行为）或条件风险价值（CVaR）。

实现分布式强化学习需要修改网络的输出层，以预测分布参数（原子的概率或分位数），并相应地调整损失函数（KL散度或分位数回归损失）和贝尔曼更新机制。尽管增加了复杂度，但其经验性增益和处理风险的能力使其成为深度强化学习中的一项重要进展。